{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import AnnotatorModel, AnnotatorType\n",
    "from pyspark.sql.functions import col, when\n",
    "import random\n",
    "\n",
    "\n",
    "class ErrorTransformer(AnnotatorModel):\n",
    "    inputAnnotatorTypes = [AnnotatorType.DOCUMENT]\n",
    "    outputAnnotatorType = AnnotatorType.DOCUMENT\n",
    "\n",
    "    def __init__(self, fraction):\n",
    "        super(ErrorTransformer, self).__init__(\n",
    "            classname=None,\n",
    "            java_model=None,\n",
    "        )\n",
    "        self.fraction = fraction\n",
    "        self.num_affected_rows = 0\n",
    "        self.affected_rows_indices = []\n",
    "\n",
    "\n",
    "class CategoricalLabelTransformer(ErrorTransformer):\n",
    "    def __init__(self, fraction, categories=None):\n",
    "        super(CategoricalLabelTransformer, self).__init__(fraction=fraction)\n",
    "        self.categories = categories\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        self.inputCol = self.getInputCols()[0]\n",
    "        self.outputCol = self.getOutputCol()\n",
    "        # Determine the number of affected rows\n",
    "        num_rows = dataset.count()\n",
    "        print(\"num_rows:\", num_rows)\n",
    "\n",
    "        # Select random rows to transform\n",
    "        affected_rows = dataset.sample(False, self.fraction)\n",
    "        self.num_affected_rows = affected_rows.count()\n",
    "        print(\"affected_rows_count:\", self.num_affected_rows)\n",
    "\n",
    "        print(affected_rows.groupBy(\"language\").count().show())\n",
    "\n",
    "        replaced_column = (\n",
    "            when(col(self.inputCol) == \"en\", self._select_random_category(\"en\"))\n",
    "            .when(col(self.inputCol) == \"es\", self._select_random_category(\"es\"))\n",
    "            .when(col(self.inputCol) == \"tr\", self._select_random_category(\"tr\"))\n",
    "            .when(col(self.inputCol) == \"fr\", self._select_random_category(\"fr\"))\n",
    "            .when(col(self.inputCol) == \"de\", self._select_random_category(\"de\"))\n",
    "            .otherwise(col(self.inputCol))\n",
    "        )\n",
    "\n",
    "        # Apply transformation to selected rows\n",
    "        transformed_rows = affected_rows.withColumn(self.inputCol, replaced_column)\n",
    "\n",
    "        print(\"transformed_rows:\", transformed_rows.show(50))\n",
    "        print(\"transformed_rows count:\", transformed_rows.count())\n",
    "\n",
    "        # replace affected_rows in dataset for transformed_rows\n",
    "        substract_df = dataset.subtract(affected_rows)\n",
    "        print(\"dataset count:\", dataset.count())\n",
    "        union_df = substract_df.union(transformed_rows)\n",
    "\n",
    "        return union_df\n",
    "\n",
    "    def _select_random_category(self, label):\n",
    "        other_categories = self.categories.copy()\n",
    "        other_categories.remove(label)\n",
    "        category = random.choice(other_categories)\n",
    "        print(f\"selecting random category! {category}\")\n",
    "        return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start(apple_silicon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 52500\n",
      "affected_rows_count: 5214\n",
      "+--------+-----+\n",
      "|language|count|\n",
      "+--------+-----+\n",
      "|      en| 1047|\n",
      "|      tr| 1076|\n",
      "|      de| 1047|\n",
      "|      es| 1046|\n",
      "|      fr|  998|\n",
      "+--------+-----+\n",
      "\n",
      "None\n",
      "selecting random category! fr\n",
      "selecting random category! tr\n",
      "selecting random category! en\n",
      "selecting random category! tr\n",
      "selecting random category! es\n",
      "+--------------------+--------+-----+-----+\n",
      "|         review_body|language|stars|index|\n",
      "+--------------------+--------+-----+-----+\n",
      "|No se el resultad...|      tr|    4|    6|\n",
      "|*Finally!* is all...|      fr|    4|   17|\n",
      "|Bin sehr zufriede...|      es|    5|   23|\n",
      "|Las tallas chinas...|      tr|    1|   38|\n",
      "|...das leider imm...|      es|    3|   60|\n",
      "|Se pega sobre si ...|      tr|    1|   84|\n",
      "|ürünü dahayeni ku...|      en|    5|   99|\n",
      "|fiyatı daha uygun...|      en|    3|  106|\n",
      "|Calidad en genera...|      tr|    3|  119|\n",
      "|Ürün genel perfor...|      en|    3|  128|\n",
      "|Sieht gut aus (fü...|      es|    3|  130|\n",
      "|I was looking for...|      fr|    5|  133|\n",
      "|başka mağaza ve s...|      en|    4|  156|\n",
      "|Limpia muy bien p...|      tr|    4|  160|\n",
      "|En móvil en si es...|      tr|    3|  172|\n",
      "|Me encanta,recoge...|      tr|    5|  173|\n",
      "|Meine kleine Maus...|      es|    4|  179|\n",
      "|L1 funktioniert n...|      es|    2|  186|\n",
      "|Aldım kullanıyoru...|      en|    5|  230|\n",
      "|Ich habe naturblo...|      es|    2|  236|\n",
      "|This product is g...|      fr|    1|  238|\n",
      "|Es fácil de lleva...|      tr|    3|  261|\n",
      "|paranızın tam kar...|      en|    5|  290|\n",
      "|No me ha gustado ...|      tr|    1|  329|\n",
      "|J'ai reçus les de...|      tr|    1|  336|\n",
      "|J'ai bien reçu à ...|      tr|    2|  372|\n",
      "|Es el primer libr...|      tr|    3|  376|\n",
      "|Bonjour sur cet a...|      tr|    1|  381|\n",
      "|Maalesef kalıcı o...|      en|    2|  391|\n",
      "|Camión sencillo p...|      tr|    4|  396|\n",
      "|Loved this produc...|      fr|    5|  399|\n",
      "|sigara çay ve kah...|      en|    5|  407|\n",
      "|Aunque venia forr...|      tr|    1|  410|\n",
      "|Son muy cómodas ¡...|      tr|    4|  415|\n",
      "|La vitre arrière ...|      tr|    4|  420|\n",
      "|Süße Sticker! Mei...|      es|    5|  421|\n",
      "|Good for lunches,...|      fr|    4|  425|\n",
      "|Kein DAB in Deuts...|      es|    1|  431|\n",
      "|+ Urun 24 saat ic...|      en|    5|  434|\n",
      "|Kullanımı basit; ...|      en|    4|  450|\n",
      "|Alas, the product...|      fr|    2|  452|\n",
      "|Très bonne qualit...|      tr|    4|  453|\n",
      "|Batería original ...|      tr|    5|  458|\n",
      "|Las fechas de ent...|      tr|    1|  474|\n",
      "|Bought this to go...|      fr|    3|  519|\n",
      "|No me ha llegado ...|      tr|    1|  520|\n",
      "|Exactly as descri...|      fr|    5|  550|\n",
      "|Facile à installe...|      tr|    4|  553|\n",
      "|HB ye çok teşekkü...|      en|    5|  556|\n",
      "|de simple \"mines\"...|      tr|    5|  568|\n",
      "+--------------------+--------+-----+-----+\n",
      "only showing top 50 rows\n",
      "\n",
      "transformed_rows: None\n",
      "transformed_rows count: 5214\n",
      "dataset count: 52500\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Define the CategoricalLabelTransformer annotator\n",
    "cat_label_transformer = (\n",
    "    CategoricalLabelTransformer(\n",
    "        fraction=0.1,\n",
    "        categories=[\"en\", \"es\", \"tr\", \"fr\", \"de\"],\n",
    "        # categories=[\"en\"],\n",
    "    )\n",
    "    .setInputCols([\"language\"])\n",
    "    .setOutputCol(\"language\")\n",
    ")\n",
    "\n",
    "# Create the SparkNLP pipeline\n",
    "pipeline = Pipeline(stages=[cat_label_transformer])\n",
    "data = spark.read.option(\"header\", True).csv(\n",
    "    \"./data/test_data_consolidated.csv\", escape='\"'\n",
    ")\n",
    "\n",
    "df_with_index = data.withColumn(\"index\", monotonically_increasing_id())\n",
    "# Create the SparkNLP pipeline\n",
    "pipeline = Pipeline(stages=[cat_label_transformer])\n",
    "\n",
    "# Fit the pipeline to your data\n",
    "model = pipeline.fit(df_with_index)\n",
    "\n",
    "# Transform your data\n",
    "transformed_data = model.transform(df_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52500"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 564:>                                                        (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|language|count|\n",
      "+--------+-----+\n",
      "|      en|10558|\n",
      "|      tr|11494|\n",
      "|      de| 9553|\n",
      "|      es|10364|\n",
      "|      fr|10531|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_data.groupBy(\"language\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/01 14:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     state|index|\n",
      "+----------+-----+\n",
      "|   Alabama|    0|\n",
      "|California|    1|\n",
      "|     Maine|    2|\n",
      "|      Ohio|    3|\n",
      "|   Arizona|    4|\n",
      "|   Montana|    5|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+----------+\n",
      "|     state|state_code|\n",
      "+----------+----------+\n",
      "|   Alabama|        S0|\n",
      "|California|        S1|\n",
      "|     Maine|        S2|\n",
      "|      Ohio|        S3|\n",
      "|   Arizona|        S4|\n",
      "|   Montana|        S5|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"State_Prefix\").getOrCreate()\n",
    "\n",
    "\n",
    "def get_state_codes(input_df, prefix=None):\n",
    "    if prefix is not None:\n",
    "        output_df = input_df.withColumn(\n",
    "            \"state_code\", F.concat(F.lit(prefix), F.col(\"index\"))\n",
    "        )\n",
    "    else:\n",
    "        output_df = input_df.withColumn(\"state_code\", F.col(\"index\"))\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "data = {\"state\": [\"Alabama\", \"California\", \"Maine\", \"Ohio\", \"Arizona\", \"Montana\"]}\n",
    "df1 = spark.createDataFrame(pd.DataFrame(data))\n",
    "\n",
    "df1 = df1.withColumn(\n",
    "    \"index\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1\n",
    ")\n",
    "df1.show()\n",
    "\n",
    "df1 = get_state_codes(df1, \"S\")\n",
    "df1 = df1.drop(\"index\")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Transformer\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame, Row\n",
    "import random\n",
    "\n",
    "\n",
    "class CategoricalLabelTransformer(\n",
    "    Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable\n",
    "):\n",
    "    def __init__(self, inputCol=None, outputCol=None, fraction=0.1, categories=None):\n",
    "        super(CategoricalLabelTransformer, self).__init__()\n",
    "        self.fraction = fraction\n",
    "        self.categories = categories\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def setFraction(self, value):\n",
    "        self.fraction = value\n",
    "\n",
    "    def setCategories(self, value):\n",
    "        self.categories = value\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def corrupt_row(row):\n",
    "            if random.random() < self.fraction:\n",
    "                current_val = row[self.inputCol]\n",
    "                if isinstance(self.categories, dict):\n",
    "                    new_val = self.categories.get(current_val, current_val)\n",
    "                else:\n",
    "                    categories_min_current = [\n",
    "                        c for c in self.categories if c != current_val\n",
    "                    ]\n",
    "                    new_val = random.choice(categories_min_current)\n",
    "                return Row(**{**row.asDict(), self.outputCol: new_val})\n",
    "            return row\n",
    "\n",
    "        return dataset.rdd.map(corrupt_row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Fit and transform the pipeline\u001b[39;00m\n\u001b[1;32m     36\u001b[0m pipeline_model \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit(df)\n\u001b[0;32m---> 37\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Show the transformed DataFrame\u001b[39;00m\n\u001b[1;32m     40\u001b[0m transformed_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/ml/pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[0;32m--> 304\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "Cell \u001b[0;32mIn[108], line 39\u001b[0m, in \u001b[0;36mCategoricalLabelTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Row(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrow\u001b[38;5;241m.\u001b[39masDict(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputCol: new_val})\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrupt_row\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/university/data-preparation/multilingual-ml-project-sparknlp/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1028\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1029\u001b[0m                 error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1030\u001b[0m                 message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1031\u001b[0m             )\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.99\u001b[39m:\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.base import *\n",
    "import random\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"CategoricalLabelTransformerPipeline\"\n",
    ").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"orange\", 4), (\"banana\", 5)]\n",
    "columns = [\"fruit\", \"id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Initialize other Spark NLP components\n",
    "document_assembler = DocumentAssembler().setInputCol(\"fruit\").setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"tokens\")\n",
    "\n",
    "# Initialize the custom transformer\n",
    "cat_label_transformer = CategoricalLabelTransformer(\n",
    "    inputCol=\"fruit\",\n",
    "    outputCol=\"corrupted_fruit\",\n",
    "    fraction=0.2,\n",
    "    categories=[\"apple\", \"banana\", \"orange\"],\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, cat_label_transformer])\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "pipeline_model = pipeline.fit(df)\n",
    "transformed_df = pipeline_model.transform(df)\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "transformed_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
